I'm looking at the Kalman equations, trying to understand them. Part I, "What are all those matrices", describes the meaning of the matrices we set up: x,F, P, H, R and u.

This post describes the Kalman filter process as I understand it. I noticed, to my surprise, that P, our covariance matrix, never depends on the observed values z or the move u,so the Kalman filter converges in the same number of steps, possibly an infinite number, no matter what we observe. (Thank you to @kpalamartchouk.) As @amorfis points out below in the comments, just as with combining one dimensional gaussians, the new variance is a function of the old variances but not the old means.

First, we predict our next x:

x' = Fx + u

P is the expectation of x times (transp x). We update it to be the expectation of Fx times (transp Fx):

P' = FP(transp F)

y becomes the difference between the move and what we expected:

y = z - Hx

S is the covariance of the move, adding up the covariance in move space of the position and the covariance of the measurement:

S = HP(transp H) + R

Now I start to wave my hands. I assume this next matrix is called K because this is the work of the Kalman filter. Whatever is happening here, it doesn't depend on u or z. We're computing how much of the difference between the observed move and the expected move to add to x.

K = P (transp H) (inv S)

We update x:

x' = x + Ky

And we subtract some uncertainty from P, again not depending on u or z:

P' = P - P(transp H)(inv S)HP

Our input doesn't affect how fast we converge.
